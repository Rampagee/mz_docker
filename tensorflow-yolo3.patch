diff --git a/.gitignore b/.gitignore
index 0001c73..39abcd7 100644
--- a/.gitignore
+++ b/.gitignore
@@ -28,3 +28,7 @@ live_hot_statistic.py
 /test_model/*
 /model_data/darknet53.weights
 /model_data/yolov3.weights
+
+zebra.ini
+zebra.log
+**.jpg
diff --git a/config.py b/config.py
index 8e57a09..5fbfa39 100644
--- a/config.py
+++ b/config.py
@@ -25,7 +25,7 @@ gpu_index = "0"
 log_dir = './logs'
 data_dir = './model_data'
 model_dir = './test_model/model.ckpt-192192'
-pre_train_yolo3 = False
+pre_train_yolo3 = True
 yolo3_weights_path = './model_data/yolov3.weights'
 darknet53_weights_path = './model_data/darknet53.weights'
 anchors_path = './model_data/yolo_anchors.txt'
@@ -34,3 +34,9 @@ train_data_file = '/data0/dataset/coco/train2017'
 val_data_file = '/data0/dataset/coco/val2017'
 train_annotations_file = '/data0/gaochen3/tensorflow-yolo3/annotations/instances_train2017.json'
 val_annotations_file = '/data0/gaochen3/tensorflow-yolo3/annotations/instances_val2017.json'
+
+USE_TF = False
+INHIBIT_NMS = False
+save_pb = False
+pb_yolo3 = True
+pb_path = "./yolov3.pb"
diff --git a/cv2utils.py b/cv2utils.py
new file mode 100644
index 0000000..93a6ba2
--- /dev/null
+++ b/cv2utils.py
@@ -0,0 +1,40 @@
+# -*- coding: utf-8 -*-
+
+import numpy as np
+import cv2
+import mipso_custom
+
+def print_text(frame, x, y, text, color, fontScale=0.5, lineType=1):
+    font                   = cv2.FONT_HERSHEY_SIMPLEX
+
+    cv2.putText(frame, text, (x, y),
+        font,
+        fontScale,
+        color,
+        lineType)
+
+def letterbox_image(image, size):
+    """
+    Introduction
+    ------------
+        对预测输入图像进行缩放，按照长宽比进行缩放，不足的地方进行填充
+    Parameters
+    ----------
+        image: 输入图像
+        size: 图像大小
+    Returns
+    -------
+        boxed_image: 缩放后的图像
+    """
+    image_w, image_h = image.shape[1], image.shape[0]
+    w, h = size
+    new_w = int(image_w * min(w*1.0/image_w, h*1.0/image_h))
+    new_h = int(image_h * min(w*1.0/image_w, h*1.0/image_h))
+    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
+    resized_image = cv2.resize(image, (new_w,new_h))
+
+    boxed_image = np.empty((w, h, 3))
+    boxed_image.fill(128)
+
+    boxed_image[(h-new_h)//2:(h-new_h)//2+new_h, (w-new_w)//2:(w-new_w)//2+new_w] = resized_image
+    return boxed_image
diff --git a/detect.py b/detect.py
index e13a310..ada414b 100644
--- a/detect.py
+++ b/detect.py
@@ -1,16 +1,233 @@
+# -*- coding: utf-8 -*-
+
 import os
 import config
 import argparse
 import numpy as np
 import tensorflow as tf
-from yolo_predict import yolo_predictor
 from PIL import Image, ImageFont, ImageDraw
 from utils import letterbox_image, load_weights
+import cv2
+import cv2utils
+import time
+import mipso_custom
+
+if config.USE_TF:
+    from yolo_predict_tf import yolo_predictor
+else:
+    from yolo_predict import yolo_predictor
 
 # 指定使用GPU的Index
 os.environ["CUDA_VISIBLE_DEVICES"] = config.gpu_index
 
-def detect(image_path, model_path, yolo_weights = None):
+def _get_io_layers(graph_def):
+    """Retrieve the input and output layers from a tensorflow GraphDef.
+    Returns a tuple of the list of input nodes and the list of output layers' name.
+    """
+    inputs = [] # list entry nodes (placeholders)
+    nodes = set() # list all nodes
+    full_inputs = set() # list nodes that are inputs of others
+    for n in graph_def.node:
+        if n.op == "Assign":
+            # add inputs but ignore it as possible output
+            for inp in n.input:
+                nodes.add(inp)
+                full_inputs.add(inp)
+        elif n.op == "VariableV2":
+            nodes.add(n.name)
+            full_inputs.add(n.name)
+        elif n.op != "Const" and n.op != "NoOp":
+            nodes.add(n.name)
+            for inp in n.input:
+                nodes.add(inp)
+                full_inputs.add(inp)
+            if len(n.input) == 0:
+                inputs.append(n)
+    outputs = nodes - full_inputs
+    return inputs, list(outputs)
+
+def read_image(image_path):
+    image = Image.open(image_path)
+    resize_image = letterbox_image(image, (config.input_shape, config.input_shape))
+    image_data = np.array(resize_image, dtype = np.float32)
+    image_data /= 255.
+    return image, np.expand_dims(image_data, axis = 0)
+
+
+def draw_out_image(image, predictor, out_path, out_boxes, out_scores, out_classes):
+    font = ImageFont.truetype(font = 'font/FiraMono-Medium.otf', size = np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))
+    thickness = max(2, (image.size[0] + image.size[1] ) // 600)
+
+    for i, c in reversed(list(enumerate(out_classes))):
+        predicted_class = predictor.class_names[c]
+        box = out_boxes[i]
+        score = out_scores[i]
+
+        label = '{} {:.2f}'.format(predicted_class, score)
+        draw = ImageDraw.Draw(image)
+        label_size = draw.textsize(label, font)
+
+        top, left, bottom, right = box
+        top = max(0, np.floor(top + 0.5).astype('int32'))
+        left = max(0, np.floor(left + 0.5).astype('int32'))
+        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))
+        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))
+        if i > len(out_boxes) - 20:
+            print(label, (left, top), (right, bottom))
+        if i == len(out_boxes) - 20:
+            print("Stop here, {} more entries not listed...".format(len(out_boxes) - 20))
+
+        if top - label_size[1] >= 0:
+            text_origin = np.array([left, top - label_size[1]])
+        else:
+            text_origin = np.array([left, top + 1])
+
+        # My kingdom for a good redistributable image drawing library.
+        for i in range(thickness):
+            draw.rectangle(
+                [left + i, top + i, right - i, bottom - i],
+                outline = predictor.colors[c])
+        draw.rectangle(
+            [tuple(text_origin), tuple(text_origin + label_size)],
+            fill = predictor.colors[c])
+        draw.text(text_origin, label, fill=(0, 0, 0), font=font)
+        del draw
+    image.show()
+    image.save(out_path)
+
+
+def run_detect(sess, capture, input_image, output, batch, out_path=None, loop=False):
+    predictor = yolo_predictor(config.obj_threshold, config.nms_threshold, config.classes_path, config.anchors_path)
+    image_data_batch = []
+    capture_image = []
+    first_capture = 1
+    firstShow = True
+    while len(capture) > 0:
+        for c in capture:
+            _, image = c.read()
+            if loop and image is None:
+                c.set(cv2.CAP_PROP_POS_FRAMES, 0)
+                _, image = c.read()
+
+            if first_capture and out_path is not None:
+                assert (len(capture) == 1), 'saving video works only with single input'
+                out_image = []
+                height, width, _ = image.shape
+                fourcc = cv2.VideoWriter_fourcc(*'XVID')
+                fps = max(1, round(capture[0].get(cv2.CAP_PROP_FPS)))
+                videoWriter = cv2.VideoWriter(out_path, fourcc, fps, (width, height))
+                first_capture = 0
+
+            if image is None:
+                capture = []
+                if len(image_data_batch) > 0:
+                    batch = 1
+            else:
+                capture_image.append(image)
+                resize_image = cv2utils.letterbox_image(image, (config.input_shape,config.input_shape))
+
+                image_data = np.array(resize_image, dtype = np.float32)
+                image_data /= 255.
+                image_data_batch.append(image_data)
+
+        if len(image_data_batch) < batch:
+            continue
+
+        if config.USE_TF:
+            out_boxes, out_scores, out_classes = sess.run(
+                [boxes, scores, classes],
+                feed_dict={
+                    input_image: image_data,
+                    input_image_shape: [image.shape[0], image.shape[1]]
+                })
+        else:
+            start_inf = time.time()
+            inf = sess.run(output, feed_dict={input_image: image_data_batch})
+            took_inf = time.time() - start_inf
+            out_boxes_batch = []
+            out_scores_batch = []
+            out_classes_batch = []
+            for idx, image in enumerate(capture_image):
+                inf_l = [ np.expand_dims(x[idx], axis = 0) for x in inf]
+                out_boxes, out_scores, out_classes = predictor.eval(inf_l, [image.shape[0], image.shape[1]], max_boxes = 20)
+                out_boxes_batch.append(out_boxes)
+                out_scores_batch.append(out_scores)
+                out_classes_batch.append(out_classes)
+
+            if config.save_pb:
+                config.save_pb = False # do it once
+                output_graph_def = tf.graph_util.convert_variables_to_constants(sess, tf.get_default_graph().as_graph_def(), [o.name[:-2] for o in output])
+                tf.train.write_graph(output_graph_def, './', config.pb_path, False)
+
+        # print and show result
+        for idx,(image, out_boxes, out_scores, out_classes) in enumerate(zip(capture_image, out_boxes_batch, out_scores_batch, out_classes_batch)):
+            print('Found {} boxes for {}'.format(len(out_boxes), 'img'))
+            thickness = max(1, (image.shape[1] + image.shape[0]) // 600)
+
+            for i, c in reversed(list(enumerate(out_classes))):
+                predicted_class = predictor.class_names[c]
+                box = out_boxes[i]
+                score = out_scores[i]
+                color = predictor.colors[c][::-1]
+
+                label = '{} {:.2f}'.format(predicted_class, score)
+
+                top, left, bottom, right = box
+                top = max(0, np.floor(top + 0.5).astype('int32'))
+                left = max(0, np.floor(left + 0.5).astype('int32'))
+                bottom = min(image.shape[0], np.floor(bottom + 0.5).astype('int32'))
+                right = min(image.shape[1], np.floor(right + 0.5).astype('int32'))
+                if i > len(out_boxes) - 20:
+                    print(label, (left, top), (right, bottom))
+                if i == len(out_boxes) - 20:
+                    print("Stop here, {} more entries not listed...".format(len(out_boxes) - 20))
+
+                cv2.rectangle(image, (left, top), (right, bottom), color,
+                              thickness)
+                cv2utils.print_text(image, left, top-5, label, color)
+
+            mipso_custom.mipso_window_info(image, idx, "TF YOLOV3", took_inf)
+            if out_path is None:
+                if firstShow:
+                    height, width, _ = image.shape
+                    mipso_custom.mipso_window_init(idx, width, height)
+                cv2.imshow(str(idx), image)
+            else:
+                out_image.append(image)
+        firstShow = False
+        image_data_batch = []
+        capture_image = []
+        if out_path is None:
+            choice = cv2.waitKey(1)
+        else:
+            choice = 0
+        choice = mipso_custom.mipso_process_key(choice)
+        if choice == 27: break
+
+    if out_path is not None:
+        print("No more input, encoding capture...")
+        for v in out_image:
+            videoWriter.write(v)
+        videoWriter.release()
+
+
+def detect_pb(capture, pb_path, out_path = "./result1.jpg", batch = 1, loop=False):
+    config.save_pb = False # Do not do that now
+    tfgraph = tf.Graph()
+    graph_def = tf.GraphDef()
+    with open(pb_path, 'rb') as f:
+        graph_def.ParseFromString(f.read())
+    inputs, outputs = _get_io_layers(graph_def)
+    print("inputs :", inputs[0].name)
+    # print("outputs :", len(outputs), outputs)
+    with tfgraph.as_default():
+        tf.import_graph_def(graph_def)
+    input_image = tfgraph.get_operation_by_name("import/"+inputs[0].name).outputs[0]
+    output_ops = [tfgraph.get_operation_by_name("import/" + x).outputs[0] for x in sorted(outputs)]
+    with tf.Session(graph = tfgraph) as sess:
+        run_detect(sess, capture, input_image, output_ops, batch, out_path, loop)
+
+def detect(capture, model_path, yolo_weights = None, out_path = "./result1.jpg", batch = 1, loop=False):
     """
     Introduction
     ------------
@@ -20,75 +237,61 @@ def detect(image_path, model_path, yolo_weights = None):
         model_path: 模型路径
         image_path: 图片路径
     """
-    image = Image.open(image_path)
-    resize_image = letterbox_image(image, (416, 416))
-    image_data = np.array(resize_image, dtype = np.float32)
-    image_data /= 255.
-    image_data = np.expand_dims(image_data, axis = 0)
     input_image_shape = tf.placeholder(dtype = tf.int32, shape = (2,))
-    input_image = tf.placeholder(shape = [None, 416, 416, 3], dtype = tf.float32)
+    input_image = tf.placeholder(shape = [None, None, None, 3], dtype = tf.float32)
+    output = None
     predictor = yolo_predictor(config.obj_threshold, config.nms_threshold, config.classes_path, config.anchors_path)
-    boxes, scores, classes = predictor.predict(input_image, input_image_shape)
+    if config.USE_TF:
+        boxes, scores, classes = predictor.predict(input_image, input_image_shape)
     with tf.Session() as sess:
         if yolo_weights is not None:
             with tf.variable_scope('predict'):
-                boxes, scores, classes = predictor.predict(input_image, input_image_shape)
+                if config.USE_TF:
+                    boxes, scores, classes = predictor.predict(input_image, input_image_shape)
+                else:
+                    output = predictor.predict(input_image)
             load_op = load_weights(tf.global_variables(scope = 'predict'), weights_file = yolo_weights)
             sess.run(load_op)
         else:
             saver = tf.train.Saver()
             saver.restore(sess, model_path)
-        out_boxes, out_scores, out_classes = sess.run(
-            [boxes, scores, classes],
-            feed_dict={
-                input_image: image_data,
-                input_image_shape: [image.size[1], image.size[0]]
-            })
-        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))
-        font = ImageFont.truetype(font = 'font/FiraMono-Medium.otf', size = np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))
-        thickness = (image.size[0] + image.size[1]) // 300
-
-        for i, c in reversed(list(enumerate(out_classes))):
-            predicted_class = predictor.class_names[c]
-            box = out_boxes[i]
-            score = out_scores[i]
-
-            label = '{} {:.2f}'.format(predicted_class, score)
-            draw = ImageDraw.Draw(image)
-            label_size = draw.textsize(label, font)
-
-            top, left, bottom, right = box
-            top = max(0, np.floor(top + 0.5).astype('int32'))
-            left = max(0, np.floor(left + 0.5).astype('int32'))
-            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))
-            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))
-            print(label, (left, top), (right, bottom))
-
-            if top - label_size[1] >= 0:
-                text_origin = np.array([left, top - label_size[1]])
-            else:
-                text_origin = np.array([left, top + 1])
+        run_detect(sess, capture, input_image, output, batch, out_path, loop)
 
-            # My kingdom for a good redistributable image drawing library.
-            for i in range(thickness):
-                draw.rectangle(
-                    [left + i, top + i, right - i, bottom - i],
-                    outline = predictor.colors[c])
-            draw.rectangle(
-                [tuple(text_origin), tuple(text_origin + label_size)],
-                fill = predictor.colors[c])
-            draw.text(text_origin, label, fill=(0, 0, 0), font=font)
-            del draw
-        image.show()
-        image.save('./result1.jpg')
 
 if __name__ == '__main__':
     parser = argparse.ArgumentParser(argument_default = argparse.SUPPRESS)
     parser.add_argument(
-        '--image_file', type = str, help = 'image file path'
+        '--image_file', type = str, help = 'image file path', default="dog.jpg"
+    )
+    parser.add_argument(
+        '--inputSize', type = str, help = 'input dimension for the network (must be square)', default="416x416"
+    )
+    parser.add_argument(
+        '--batch', type = int, help = 'batch size', default=1
+    )
+    parser.add_argument(
+        '--out_file', type = str, help = 'output image path.', default=None
+    )
+    parser.add_argument(
+        '--loop', action="store_true", help = 'loop input video.', default=False
+    )
+    parser.add_argument(
+        '--save_pb', action="store_true", help = 'freeze models and save pb.', default=None
     )
     FLAGS = parser.parse_args()
-    if config.pre_train_yolo3 == True:
-        detect(FLAGS.image_file, config.model_dir, config.yolo3_weights_path)
+
+    if FLAGS.save_pb is not None:
+        config.pb_yolo3 = False
+        config.save_pb = True
+
+    config.input_shape = int(FLAGS.inputSize.split('x')[0])
+    assert config.input_shape == int(FLAGS.inputSize.split('x')[1]), "ERROR: input dimension must be square"
+
+    capture = mipso_custom.mipso_opencv_capture(FLAGS.image_file)
+
+    if config.pb_yolo3 == True:
+        detect_pb(capture, config.pb_path, out_path=FLAGS.out_file, batch=FLAGS.batch, loop=FLAGS.loop)
+    elif config.pre_train_yolo3 == True:
+        detect(capture, config.model_dir, config.yolo3_weights_path, out_path=FLAGS.out_file, batch=FLAGS.batch, loop=FLAGS.loop)
     else:
-        detect(FLAGS.image_file, config.model_dir)
+        detect(capture, config.model_dir, out_path=FLAGS.out_file, loop=FLAGS.loop)
diff --git a/install b/install
new file mode 100755
index 0000000..705a99c
--- /dev/null
+++ b/install
@@ -0,0 +1,5 @@
+#! /bin/bash
+
+python3 -m pip install --user --upgrade pip
+python3 -m pip install --user tensorflow==1.14.0 opencv-python pillow
+
diff --git a/mipso_custom.py b/mipso_custom.py
new file mode 100644
index 0000000..69a97ef
--- /dev/null
+++ b/mipso_custom.py
@@ -0,0 +1,232 @@
+
+import cv2
+import os
+
+############################################################
+############################################################
+
+# the application will open several window and place them on the desktop at
+# specific location. The below variables allow to configure the location
+
+# this is the X/Y offset for window position
+WINDOW_X_OFFSET         = int(os.environ.get("WINDOW_X_OFFSET")         or 0)
+WINDOW_Y_OFFSET         = int(os.environ.get("WINDOW_Y_OFFSET")         or 0)
+
+# this is the number of window per row
+WINDOW_PER_ROW          = int(os.environ.get("WINDOW_PER_ROW")          or 3)
+
+# this is a scaling factor to reduce the window dimension (2: window will be 2x smaller)
+WINDOW_RATIO            = float(os.environ.get("WINDOW_RATIO")          or 1)
+
+# this force the width and height of the video
+WINDOW_WIDTH            = int(os.environ.get("WINDOW_WIDTH")            or 0)
+WINDOW_HEIGHT           = int(os.environ.get("WINDOW_HEIGHT")           or 0)
+
+# this is the index position to place a window so a slide can be displayed. For instance, in a 3x3 configuration use 4 to let the center empty
+WINDOW_SLIDE_IDX        = int(os.environ.get("WINDOW_SLIDE_IDX")        or 4)
+
+# this is the number of batch (usually the number of cores) per system
+BATCH_PER_SYSTEM        = int(os.environ.get("BATCH_PER_SYSTEM")        or 1)
+
+# this is the number of system per board
+SYSTEM_PER_BOARD        = int(os.environ.get("SYSTEM_PER_BOARD")        or 2)
+
+# show info/FPS by default
+SHOW_INFO               = int(os.environ.get("SHOW_INFO")               or 0)
+SHOW_FPS                = int(os.environ.get("SHOW_FPS")                or 0)
+
+
+# enable the MJPG mode
+OPENCV_CAMERA_MJPG      = int(os.environ.get("OPENCV_CAMERA_MJPG")      or 1)
+# force camera width/height
+OPENCV_CAMERA_WIDTH     = int(os.environ.get("OPENCV_CAMERA_WIDTH")     or 0)
+OPENCV_CAMERA_HEIGHT    = int(os.environ.get("OPENCV_CAMERA_HEIGHT")    or 0)
+
+
+############################################################
+############################################################
+
+# only for YoloV2:
+def mipso_get_interpolation():
+    return cv2.INTER_NEAREST
+    return cv2.INTER_LINEAR
+    #return cv2.INTER_LANCZOS4
+
+# only for YoloV2
+def mipso_camera_reduce_delay():
+    return False # normal execution: capture / inference / display from capture
+    return True # reduce delay execution: inference on prev capture / capture / display from capture using previous inference
+
+# only for YoloV2
+def mipso_box_rounding(i):
+    return i
+    round = 20
+    return ((i + round) // (2*round) ) * 2*round
+
+def mipso_camera_init(c):
+    print ("Initial camera resolution {}x{} @ {}".format(
+        c.get(cv2.CAP_PROP_FRAME_WIDTH),
+        c.get(cv2.CAP_PROP_FRAME_HEIGHT),
+        c.get(cv2.CAP_PROP_FPS)))
+    if OPENCV_CAMERA_WIDTH != 0:
+        c.set(cv2.CAP_PROP_FRAME_WIDTH , OPENCV_CAMERA_WIDTH)
+    if OPENCV_CAMERA_HEIGHT != 0:
+        c.set(cv2.CAP_PROP_FRAME_HEIGHT , OPENCV_CAMERA_HEIGHT)
+    if OPENCV_CAMERA_MJPG != 0:
+        c.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M', 'J', 'P', 'G'))
+    print ("Using camera resolution {}x{} @ {}".format(
+        c.get(cv2.CAP_PROP_FRAME_WIDTH),
+        c.get(cv2.CAP_PROP_FRAME_HEIGHT),
+        c.get(cv2.CAP_PROP_FPS)))
+
+def mipso_window_init(idx, width, height):
+    cv2.namedWindow(str(idx), cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_FREERATIO)
+    #cv2.namedWindow(str(idx), cv2.WINDOW_NORMAL)
+
+    if WINDOW_WIDTH != 0 and WINDOW_HEIGHT != 0:
+        cur_width = WINDOW_WIDTH
+        cur_height = WINDOW_HEIGHT
+    else:
+        cur_width = int(width // WINDOW_RATIO)
+        cur_height = int(height // WINDOW_RATIO)
+
+    cv2.resizeWindow(str(idx), (cur_width, cur_height))
+    board=0
+    system=0
+    core=0
+    if 'ZEBRA_RUNSESSION_ENABLECORES' in os.environ:
+        c = os.environ['ZEBRA_RUNSESSION_ENABLECORES'].split(':')[0].split('_')
+        if len(c[0]) > 0:
+            board = int(c[0][1:])
+        if len(c) > 1:
+            system = int(c[1][1:])
+        if len(c) > 2:
+            core = int(c[2][1:])
+
+    cur_idx = idx + core + BATCH_PER_SYSTEM * ( system + SYSTEM_PER_BOARD * board)
+    # in case we want to put slide somewhere
+    if cur_idx >= WINDOW_SLIDE_IDX:
+        cur_idx = cur_idx + 1
+    pos_x = WINDOW_X_OFFSET + int( (cur_idx % WINDOW_PER_ROW)*cur_width)
+    pos_y = WINDOW_Y_OFFSET + int( 0 + (cur_idx // WINDOW_PER_ROW)*cur_height)
+
+    print("Move window {} {}x{} for system {} @ ({} , {})".format(idx, cur_width, cur_height, system, pos_x, pos_y))
+    cv2.moveWindow(str(idx), pos_x, pos_y)
+
+def mipso_parse_input(file):
+    mode='file'
+    for part in file.split(':'):
+        if 'camera' == part:
+            mode='camera'
+            continue
+        if part.isdigit():
+            yield mode, int(part)
+        else:
+            mode='file'
+            yield mode, part
+
+
+
+def mipso_opencv_capture(file):
+    """
+    return a list of opencv capture device
+    """
+    camera = []
+    # TODO: use a better handling of the different mode (class)
+    if 'camera' == file:
+        # open all camera
+        camera_idx=0
+        while True:
+            c = cv2.VideoCapture(camera_idx)
+            if c.isOpened() == False:
+                break
+            camera.append(c)
+            mipso_camera_init(c)
+            camera_idx = camera_idx + 1
+    else:
+        for mode, url in mipso_parse_input(file):
+            c = cv2.VideoCapture(url)
+            if c.isOpened() == False:
+                print("Error opening source {} {}".format(mode, url))
+                break
+            if mode == 'camera':
+                mipso_camera_init(c)
+            camera.append(c)
+
+    assert len(camera) > 0, 'Cannot capture source'
+    return camera
+
+# TODO: create a Mipso class
+showHelp = False
+pauseMode = False
+showWindowInfo = SHOW_INFO != 0
+showFps = SHOW_FPS != 0
+
+def mipso_process_key(key):
+    global showHelp
+    global pauseMode
+    global showWindowInfo
+    global showFps
+
+    if pauseMode:
+        print ("pause mode, press 'p' to play or any other key to move to the next frame")
+    while True:
+        if key == 27:
+            return key
+        if key > 0 and chr(key) in 'h':
+            showHelp = not showHelp
+        if key > 0 and chr(key) in 'p':
+            pauseMode = not pauseMode
+        if key > 0 and chr(key) in 'i':
+            showWindowInfo = not showWindowInfo
+        if key > 0 and chr(key) in 'f':
+            showFps = not showFps
+        if not pauseMode or key != -1:
+            break
+        key = cv2.waitKey(1)
+    return key
+
+def print_help(frame):
+
+    HELP="\
+Key usage:\n\
+    ESC : exit the demo\n\
+    h   : toggle displaying this help\n\
+    p   : pause the video (any other key will do frame by frame)\n\
+    i   : toggle showing network information\n\
+    f   : toggle showing FPS\n\
+"
+    scale = 1
+    if frame.shape[1] < 1240:
+        scale = scale * frame.shape[1] / 1240
+    for y, line in enumerate(HELP.split('\n')):
+        cv2.putText(frame, line, (20, int(scale*(200 + y * 40))), cv2.FONT_HERSHEY_SIMPLEX,
+                    scale, (255,255,0), 1)
+
+
+def print_text(frame, x, y, text, scale=1.2):
+    font                   = cv2.FONT_HERSHEY_SIMPLEX
+    bottomLeftCornerOfText = (10,500)
+    fontScale              = scale
+    fontColor              = (0,0,255)
+    lineType               = 2 if scale >= 1 else 1
+
+    cv2.putText(frame, text, (x, y),
+        font,
+        fontScale,
+        fontColor,
+        lineType)
+
+def mipso_window_info(f, idx, name, latency):
+    if showHelp:
+        print_help(f)
+    if showWindowInfo:
+        scale = 1
+        if f.shape[1] < 500:
+            scale = scale * f.shape[1] / 500
+        if showFps:
+            print_text( f, 10, int(scale*30), "Zebra: {} FPS={}".format(name, str(int(1/latency))), scale )
+        else:
+            print_text( f, 10, int(scale*30), "Zebra: {}".format(name), scale)
+        #print_text(f, 10, 50, "Latency is: " + str(int(1000*latency)) + " ms. FPS is: " + str(int(1/latency)) + " img/s")
+
diff --git a/model/__init__.py b/model/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/model/yolo3_model.py b/model/yolo3_model.py
index 11048fa..8716fef 100644
--- a/model/yolo3_model.py
+++ b/model/yolo3_model.py
@@ -28,6 +28,7 @@ class yolo:
         self.pre_train = pre_train
         self.anchors = self._get_anchors()
         self.classes = self._get_class()
+        self.v14 = tf.VERSION >= "1.4" # leaky relu only available from 1.4
 
 
     def _get_class(self):
@@ -58,6 +59,13 @@ class yolo:
         return np.array(anchors).reshape(-1, 2)
 
 
+    def _leaky_relu(self, input_layer, alpha = 0.1):
+        if self.v14:
+            return tf.nn.leaky_relu(input_layer, alpha = alpha)
+        else:
+            m = tf.multiply(input_layer, alpha)
+            return tf.maximum(input_layer, m)
+
 
     def _batch_normalization_layer(self, input_layer, name = None, training = True, norm_decay = 0.99, norm_epsilon = 1e-3):
         '''
@@ -78,7 +86,7 @@ class yolo:
         bn_layer = tf.layers.batch_normalization(inputs = input_layer,
             momentum = norm_decay, epsilon = norm_epsilon, center = True,
             scale = True, training = training, name = name)
-        return tf.nn.leaky_relu(bn_layer, alpha = 0.1)
+        return self._leaky_relu(bn_layer, alpha = 0.1)
 
 
     def _conv2d_layer(self, inputs, filters_num, kernel_size, name, use_bias = False, strides = 1):
diff --git a/run b/run
new file mode 100755
index 0000000..41f1b9b
--- /dev/null
+++ b/run
@@ -0,0 +1,4 @@
+#! /bin/bash
+
+python3 ./detect.py --image_file "$@"
+
diff --git a/utils.py b/utils.py
index ac1c834..038cd63 100644
--- a/utils.py
+++ b/utils.py
@@ -1,3 +1,5 @@
+# -*- coding: utf-8 -*-
+
 import json
 import numpy as np
 import tensorflow as tf
@@ -138,4 +140,4 @@ def voc_ap(rec, prec):
     ap = 0.0
     for i in i_list:
         ap += ((mrec[i] - mrec[i - 1]) * mpre[i])
-    return ap, mrec, mpre
\ No newline at end of file
+    return ap, mrec, mpre
diff --git a/yolo_predict.py b/yolo_predict.py
index afdf627..6b491ea 100644
--- a/yolo_predict.py
+++ b/yolo_predict.py
@@ -1,3 +1,5 @@
+# -*- coding: utf-8 -*-
+
 import os
 import config
 import random
@@ -7,6 +9,57 @@ import tensorflow as tf
 from model.yolo3_model import yolo
 
 
+def np_sigmoid(x):
+    return 1. / (1. + np.exp(-x))
+
+def np_non_max_suppression(boxes, scores, max_boxes, iou_threshold):
+    # insipred by https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/
+    # if there are no boxes, return an empty list
+    if len(boxes) == 0:
+        return []
+
+    # if the bounding boxes integers, convert them to floats --
+    # this is important since we'll be doing a bunch of divisions
+    if boxes.dtype.kind == "i":
+        boxes = boxes.astype("float")
+
+    # grab the coordinates of the bounding boxes
+    x1 = boxes[:,1]
+    y1 = boxes[:,0]
+    x2 = boxes[:,3]
+    y2 = boxes[:,2]
+
+    # compute the area of the bounding boxes and sort the bounding
+    # boxes by their score
+    area = (x2 - x1) * (y2 - y1)
+    idxs = np.argsort(scores)
+    pick = []
+
+    # keep looping while some indexes still remain in the indexes list
+    while len(idxs) > 0:
+        # grab the last index in the indexes list and add the
+        # index value to the list of picked indexes
+        i = idxs[-1]
+        pick.append(i)
+
+        # Get intersection coordonates
+        xx1 = np.maximum(x1[i], x1[idxs[:-1]])
+        yy1 = np.maximum(y1[i], y1[idxs[:-1]])
+        xx2 = np.minimum(x2[i], x2[idxs[:-1]])
+        yy2 = np.minimum(y2[i], y2[idxs[:-1]])
+        inter = np.maximum(0, xx2 - xx1) * np.maximum(0, yy2 - yy1)
+
+        # compute the iou
+        iou = inter / (area[i] + area[idxs[:-1]] - inter)
+
+        # delete all indexes from the index list that have
+        idxs = np.delete(idxs, np.append([len(idxs)-1],
+                                         np.where(iou > iou_threshold)[0]))
+
+    # return only the index that were picked
+    return pick
+
+
 class yolo_predictor:
     def __init__(self, obj_threshold, nms_threshold, classes_file, anchors_file):
         """
@@ -24,7 +77,7 @@ class yolo_predictor:
         self.anchors_path = anchors_file
         self.class_names = self._get_class()
         self.anchors = self._get_anchors()
-        hsv_tuples = [(x / len(self.class_names), 1., 1.)for x in range(len(self.class_names))]
+        hsv_tuples = [(float(x) / len(self.class_names), 1., 1.)for x in range(len(self.class_names))]
         self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
         self.colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors))
         random.seed(10101)
@@ -78,33 +131,33 @@ class yolo_predictor:
         anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
         boxes = []
         box_scores = []
-        input_shape = tf.shape(yolo_outputs[0])[1 : 3] * 32
+        input_shape = np.asarray(yolo_outputs[0].shape[1 : 3]) * 32
         # 对三个尺度的输出获取每个预测box坐标和box的分数，score计算为置信度x类别概率
         for i in range(len(yolo_outputs)):
             _boxes, _box_scores = self.boxes_and_scores(yolo_outputs[i], self.anchors[anchor_mask[i]], len(self.class_names), input_shape, image_shape)
             boxes.append(_boxes)
             box_scores.append(_box_scores)
-        boxes = tf.concat(boxes, axis = 0)
-        box_scores = tf.concat(box_scores, axis = 0)
+        boxes = np.concatenate(boxes, axis = 0)
+        box_scores = np.concatenate(box_scores, axis = 0)
 
         mask = box_scores >= self.obj_threshold
-        max_boxes_tensor = tf.constant(max_boxes, dtype = tf.int32)
         boxes_ = []
         scores_ = []
         classes_ = []
         for c in range(len(self.class_names)):
-            class_boxes = tf.boolean_mask(boxes, mask[:, c])
-            class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])
-            nms_index = tf.image.non_max_suppression(class_boxes, class_box_scores, max_boxes_tensor, iou_threshold = self.nms_threshold)
-            class_boxes = tf.gather(class_boxes, nms_index)
-            class_box_scores = tf.gather(class_box_scores, nms_index)
-            classes = tf.ones_like(class_box_scores, 'int32') * c
+            class_boxes = boxes[mask[:, c]]
+            class_box_scores = box_scores[:, c][mask[:, c]]
+            if not config.INHIBIT_NMS:
+                nms_index = np_non_max_suppression(class_boxes, class_box_scores, max_boxes, iou_threshold = self.nms_threshold)
+                class_boxes = class_boxes[nms_index]
+                class_box_scores = class_box_scores[nms_index]
+            classes = np.ones(class_box_scores.shape, dtype=np.int32) * c
             boxes_.append(class_boxes)
             scores_.append(class_box_scores)
             classes_.append(classes)
-        boxes_ = tf.concat(boxes_, axis = 0)
-        scores_ = tf.concat(scores_, axis = 0)
-        classes_ = tf.concat(classes_, axis = 0)
+        boxes_   = np.concatenate(boxes_, axis = 0)
+        scores_  = np.concatenate(scores_, axis = 0)
+        classes_ = np.concatenate(classes_, axis = 0)
         return boxes_, scores_, classes_
 
 
@@ -127,9 +180,9 @@ class yolo_predictor:
         """
         box_xy, box_wh, box_confidence, box_class_probs = self._get_feats(feats, anchors, classes_num, input_shape)
         boxes = self.correct_boxes(box_xy, box_wh, input_shape, image_shape)
-        boxes = tf.reshape(boxes, [-1, 4])
+        boxes = np.reshape(boxes, [-1, 4])
         box_scores = box_confidence * box_class_probs
-        box_scores = tf.reshape(box_scores, [-1, classes_num])
+        box_scores = np.reshape(box_scores, [-1, classes_num])
         return boxes, box_scores
 
 
@@ -150,9 +203,9 @@ class yolo_predictor:
         """
         box_yx = box_xy[..., ::-1]
         box_hw = box_wh[..., ::-1]
-        input_shape = tf.cast(input_shape, dtype = tf.float32)
-        image_shape = tf.cast(image_shape, dtype = tf.float32)
-        new_shape = tf.round(image_shape * tf.reduce_min(input_shape / image_shape))
+        input_shape = input_shape.astype(dtype = np.float32)
+        image_shape = np.asarray(image_shape, dtype = np.float32)
+        new_shape = np.round(image_shape * np.amin(input_shape / image_shape))
         offset = (input_shape - new_shape) / 2. / input_shape
         scale = input_shape / new_shape
         box_yx = (box_yx - offset) * scale
@@ -160,13 +213,13 @@ class yolo_predictor:
 
         box_mins = box_yx - (box_hw / 2.)
         box_maxes = box_yx + (box_hw / 2.)
-        boxes = tf.concat([
+        boxes = np.concatenate([
             box_mins[..., 0:1],
             box_mins[..., 1:2],
             box_maxes[..., 0:1],
             box_maxes[..., 1:2]
         ], axis = -1)
-        boxes *= tf.concat([image_shape, image_shape], axis = -1)
+        boxes *= np.concatenate([image_shape, image_shape], axis = -1)
         return boxes
 
 
@@ -187,24 +240,24 @@ class yolo_predictor:
             box_xy, box_wh, box_confidence, box_class_probs
         """
         num_anchors = len(anchors)
-        anchors_tensor = tf.reshape(tf.constant(anchors, dtype=tf.float32), [1, 1, 1, num_anchors, 2])
-        grid_size = tf.shape(feats)[1:3]
-        predictions = tf.reshape(feats, [-1, grid_size[0], grid_size[1], num_anchors, num_classes + 5])
+        anchors_tensor = anchors.astype(dtype=np.float32).reshape([1, 1, 1, num_anchors, 2])
+        grid_size = feats.shape[1:3]
+        predictions = feats.reshape([-1, grid_size[0], grid_size[1], num_anchors, num_classes + 5])
         # 这里构建13*13*1*2的矩阵，对应每个格子加上对应的坐标
-        grid_y = tf.tile(tf.reshape(tf.range(grid_size[0]), [-1, 1, 1, 1]), [1, grid_size[1], 1, 1])
-        grid_x = tf.tile(tf.reshape(tf.range(grid_size[1]), [1, -1, 1, 1]), [grid_size[0], 1, 1, 1])
-        grid = tf.concat([grid_x, grid_y], axis = -1)
-        grid = tf.cast(grid, tf.float32)
+        grid_y = np.tile(np.reshape(np.arange(grid_size[0]), [-1, 1, 1, 1]), [1, grid_size[1], 1, 1])
+        grid_x = np.tile(np.reshape(np.arange(grid_size[1]), [1, -1, 1, 1]), [grid_size[0], 1, 1, 1])
+        grid = np.concatenate([grid_x, grid_y], axis = -1)
+        grid = grid.astype(np.float32)
         # 将x,y坐标归一化为占416的比例
-        box_xy = (tf.sigmoid(predictions[..., :2]) + grid) / tf.cast(grid_size[::-1], tf.float32)
+        box_xy = (np_sigmoid(predictions[..., :2]) + grid) / np.asarray(grid_size[::-1], dtype=np.float32)
         # 将w,h也归一化为占416的比例
-        box_wh = tf.exp(predictions[..., 2:4]) * anchors_tensor / tf.cast(input_shape[::-1], tf.float32)
-        box_confidence = tf.sigmoid(predictions[..., 4:5])
-        box_class_probs = tf.sigmoid(predictions[..., 5:])
+        box_wh = np.exp(predictions[..., 2:4]) * anchors_tensor / input_shape[::-1].astype(np.float32)
+        box_confidence = np_sigmoid(predictions[..., 4:5])
+        box_class_probs = np_sigmoid(predictions[..., 5:])
         return box_xy, box_wh, box_confidence, box_class_probs
 
 
-    def predict(self, inputs, image_shape):
+    def predict(self, inputs):
         """
         Introduction
         ------------
@@ -221,5 +274,6 @@ class yolo_predictor:
         """
         model = yolo(config.norm_epsilon, config.norm_decay, self.anchors_path, self.classes_path, pre_train = False)
         output = model.yolo_inference(inputs, config.num_anchors // 3, config.num_classes, training = False)
-        boxes, scores, classes = self.eval(output, image_shape, max_boxes = 20)
-        return boxes, scores, classes
\ No newline at end of file
+        # boxes, scores, classes = self.eval(output, image_shape, max_boxes = 20)
+        # return boxes, scores, classes
+        return output
diff --git a/yolo_predict_tf.py b/yolo_predict_tf.py
new file mode 100644
index 0000000..16f493e
--- /dev/null
+++ b/yolo_predict_tf.py
@@ -0,0 +1,228 @@
+# -*- coding: utf-8 -*-
+
+import os
+import config
+import random
+import colorsys
+import numpy as np
+import tensorflow as tf
+from model.yolo3_model import yolo
+
+
+class yolo_predictor:
+    def __init__(self, obj_threshold, nms_threshold, classes_file, anchors_file):
+        """
+        Introduction
+        ------------
+            初始化函数
+        Parameters
+        ----------
+            obj_threshold: 目标检测为物体的阈值
+            nms_threshold: nms阈值
+        """
+        self.obj_threshold = obj_threshold
+        self.nms_threshold = nms_threshold
+        self.classes_path = classes_file
+        self.anchors_path = anchors_file
+        self.class_names = self._get_class()
+        self.anchors = self._get_anchors()
+        hsv_tuples = [(float(x) / len(self.class_names), 1., 1.)for x in range(len(self.class_names))]
+        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
+        self.colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors))
+        random.seed(10101)
+        random.shuffle(self.colors)
+        random.seed(None)
+
+
+    def _get_class(self):
+        """
+        Introduction
+        ------------
+            读取类别名称
+        """
+        classes_path = os.path.expanduser(self.classes_path)
+        with open(classes_path) as f:
+            class_names = f.readlines()
+        class_names = [c.strip() for c in class_names]
+        return class_names
+
+    def _get_anchors(self):
+        """
+        Introduction
+        ------------
+            读取anchors数据
+        """
+        anchors_path = os.path.expanduser(self.anchors_path)
+        with open(anchors_path) as f:
+            anchors = f.readline()
+            anchors = [float(x) for x in anchors.split(',')]
+            anchors = np.array(anchors).reshape(-1, 2)
+        return anchors
+
+
+
+    def eval(self, yolo_outputs, image_shape, max_boxes = 20):
+        """
+        Introduction
+        ------------
+            根据Yolo模型的输出进行非极大值抑制，获取最后的物体检测框和物体检测类别
+        Parameters
+        ----------
+            yolo_outputs: yolo模型输出
+            image_shape: 图片的大小
+            max_boxes:  最大box数量
+        Returns
+        -------
+            boxes_: 物体框的位置
+            scores_: 物体类别的概率
+            classes_: 物体类别
+        """
+        anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
+        boxes = []
+        box_scores = []
+        input_shape = tf.shape(yolo_outputs[0])[1 : 3] * 32
+        # 对三个尺度的输出获取每个预测box坐标和box的分数，score计算为置信度x类别概率
+        for i in range(len(yolo_outputs)):
+            _boxes, _box_scores = self.boxes_and_scores(yolo_outputs[i], self.anchors[anchor_mask[i]], len(self.class_names), input_shape, image_shape)
+            boxes.append(_boxes)
+            box_scores.append(_box_scores)
+        boxes = tf.concat(boxes, axis = 0)
+        box_scores = tf.concat(box_scores, axis = 0)
+
+        mask = box_scores >= self.obj_threshold
+        max_boxes_tensor = tf.constant(max_boxes, dtype = tf.int32)
+        boxes_ = []
+        scores_ = []
+        classes_ = []
+        for c in range(len(self.class_names)):
+            class_boxes = tf.boolean_mask(boxes, mask[:, c])
+            class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])
+            if not config.INHIBIT_NMS:
+                nms_index = tf.image.non_max_suppression(class_boxes, class_box_scores, max_boxes_tensor, iou_threshold = self.nms_threshold)
+                class_boxes = tf.gather(class_boxes, nms_index)
+                class_box_scores = tf.gather(class_box_scores, nms_index)
+            classes = tf.ones_like(class_box_scores, 'int32') * c
+            boxes_.append(class_boxes)
+            scores_.append(class_box_scores)
+            classes_.append(classes)
+        boxes_ = tf.concat(boxes_, axis = 0)
+        scores_ = tf.concat(scores_, axis = 0)
+        classes_ = tf.concat(classes_, axis = 0)
+        return boxes_, scores_, classes_
+
+
+    def boxes_and_scores(self, feats, anchors, classes_num, input_shape, image_shape):
+        """
+        Introduction
+        ------------
+            将预测出的box坐标转换为对应原图的坐标，然后计算每个box的分数
+        Parameters
+        ----------
+            feats: yolo输出的feature map
+            anchors: anchor的位置
+            class_num: 类别数目
+            input_shape: 输入大小
+            image_shape: 图片大小
+        Returns
+        -------
+            boxes: 物体框的位置
+            boxes_scores: 物体框的分数，为置信度和类别概率的乘积
+        """
+        box_xy, box_wh, box_confidence, box_class_probs = self._get_feats(feats, anchors, classes_num, input_shape)
+        boxes = self.correct_boxes(box_xy, box_wh, input_shape, image_shape)
+        boxes = tf.reshape(boxes, [-1, 4])
+        box_scores = box_confidence * box_class_probs
+        box_scores = tf.reshape(box_scores, [-1, classes_num])
+        return boxes, box_scores
+
+
+    def correct_boxes(self, box_xy, box_wh, input_shape, image_shape):
+        """
+        Introduction
+        ------------
+            计算物体框预测坐标在原图中的位置坐标
+        Parameters
+        ----------
+            box_xy: 物体框左上角坐标
+            box_wh: 物体框的宽高
+            input_shape: 输入的大小
+            image_shape: 图片的大小
+        Returns
+        -------
+            boxes: 物体框的位置
+        """
+        box_yx = box_xy[..., ::-1]
+        box_hw = box_wh[..., ::-1]
+        input_shape = tf.cast(input_shape, dtype = tf.float32)
+        image_shape = tf.cast(image_shape, dtype = tf.float32)
+        new_shape = tf.round(image_shape * tf.reduce_min(input_shape / image_shape))
+        offset = (input_shape - new_shape) / 2. / input_shape
+        scale = input_shape / new_shape
+        box_yx = (box_yx - offset) * scale
+        box_hw *= scale
+
+        box_mins = box_yx - (box_hw / 2.)
+        box_maxes = box_yx + (box_hw / 2.)
+        boxes = tf.concat([
+            box_mins[..., 0:1],
+            box_mins[..., 1:2],
+            box_maxes[..., 0:1],
+            box_maxes[..., 1:2]
+        ], axis = -1)
+        boxes *= tf.concat([image_shape, image_shape], axis = -1)
+        return boxes
+
+
+
+    def _get_feats(self, feats, anchors, num_classes, input_shape):
+        """
+        Introduction
+        ------------
+            根据yolo最后一层的输出确定bounding box
+        Parameters
+        ----------
+            feats: yolo模型最后一层输出
+            anchors: anchors的位置
+            num_classes: 类别数量
+            input_shape: 输入大小
+        Returns
+        -------
+            box_xy, box_wh, box_confidence, box_class_probs
+        """
+        num_anchors = len(anchors)
+        anchors_tensor = tf.reshape(tf.constant(anchors, dtype=tf.float32), [1, 1, 1, num_anchors, 2])
+        grid_size = tf.shape(feats)[1:3]
+        predictions = tf.reshape(feats, [-1, grid_size[0], grid_size[1], num_anchors, num_classes + 5])
+        # 这里构建13*13*1*2的矩阵，对应每个格子加上对应的坐标
+        grid_y = tf.tile(tf.reshape(tf.range(grid_size[0]), [-1, 1, 1, 1]), [1, grid_size[1], 1, 1])
+        grid_x = tf.tile(tf.reshape(tf.range(grid_size[1]), [1, -1, 1, 1]), [grid_size[0], 1, 1, 1])
+        grid = tf.concat([grid_x, grid_y], axis = -1)
+        grid = tf.cast(grid, tf.float32)
+        # 将x,y坐标归一化为占416的比例
+        box_xy = (tf.sigmoid(predictions[..., :2]) + grid) / tf.cast(grid_size[::-1], tf.float32)
+        # 将w,h也归一化为占416的比例
+        box_wh = tf.exp(predictions[..., 2:4]) * anchors_tensor / tf.cast(input_shape[::-1], tf.float32)
+        box_confidence = tf.sigmoid(predictions[..., 4:5])
+        box_class_probs = tf.sigmoid(predictions[..., 5:])
+        return box_xy, box_wh, box_confidence, box_class_probs
+
+
+    def predict(self, inputs, image_shape):
+        """
+        Introduction
+        ------------
+            构建预测模型
+        Parameters
+        ----------
+            inputs: 处理之后的输入图片
+            image_shape: 图像原始大小
+        Returns
+        -------
+            boxes: 物体框坐标
+            scores: 物体概率值
+            classes: 物体类别
+        """
+        model = yolo(config.norm_epsilon, config.norm_decay, self.anchors_path, self.classes_path, pre_train = False)
+        output = model.yolo_inference(inputs, config.num_anchors // 3, config.num_classes, training = False)
+        boxes, scores, classes = self.eval(output, image_shape, max_boxes = 20)
+        return boxes, scores, classes
