diff --git a/cfg.py b/cfg.py
index ab7d587..6d35b80 100644
--- a/cfg.py
+++ b/cfg.py
@@ -154,7 +154,7 @@ def load_conv(buf, start, conv_model):
     num_w = conv_model.weight.numel()
     num_b = conv_model.bias.numel()
     conv_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b
-    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w
+    conv_model.weight.data.copy_(torch.reshape(torch.from_numpy(buf[start:start+num_w]),(conv_model.weight.shape[0],conv_model.weight.shape[1], conv_model.weight.shape[2], conv_model.weight.shape[3]))); start=start + num_w
     return start
 
 def save_conv(fp, conv_model):
@@ -172,7 +172,7 @@ def load_conv_bn(buf, start, conv_model, bn_model):
     bn_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b
     bn_model.running_mean.copy_(torch.from_numpy(buf[start:start+num_b]));  start = start + num_b
     bn_model.running_var.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b
-    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w 
+    conv_model.weight.data.copy_(torch.reshape(torch.from_numpy(buf[start:start+num_w]),(conv_model.weight.shape[0],conv_model.weight.shape[1], conv_model.weight.shape[2], conv_model.weight.shape[3]))); start=start + num_w
     return start
 
 def save_conv_bn(fp, conv_model, bn_model):
diff --git a/darknet.py b/darknet.py
index 92ab490..ea61d14 100644
--- a/darknet.py
+++ b/darknet.py
@@ -29,10 +29,10 @@ class Reorg(nn.Module):
         assert(W % stride == 0)
         ws = stride
         hs = stride
-        x = x.view(B, C, H/hs, hs, W/ws, ws).transpose(3,4).contiguous()
-        x = x.view(B, C, H/hs*W/ws, hs*ws).transpose(2,3).contiguous()
-        x = x.view(B, C, hs*ws, H/hs, W/ws).transpose(1,2).contiguous()
-        x = x.view(B, hs*ws*C, H/hs, W/ws)
+        x = x.view(B, C, H//hs, hs, W//ws, ws).transpose(3,4).contiguous()
+        x = x.view(B, C, H//hs*W//ws, hs*ws).transpose(2,3).contiguous()
+        x = x.view(B, C, hs*ws, H//hs, W//ws).transpose(1,2).contiguous()
+        x = x.view(B, hs*ws*C, H//hs, W//ws)
         return x
 
 class GlobalAvgPool2d(nn.Module):
@@ -146,7 +146,7 @@ class Darknet(nn.Module):
                 kernel_size = int(block['size'])
                 stride = int(block['stride'])
                 is_pad = int(block['pad'])
-                pad = (kernel_size-1)/2 if is_pad else 0
+                pad = (kernel_size-1)//2 if is_pad else 0
                 activation = block['activation']
                 model = nn.Sequential()
                 if batch_normalize:
diff --git a/demo.py b/demo.py
index 0a7cd88..294be78 100644
--- a/demo.py
+++ b/demo.py
@@ -1,8 +1,10 @@
 from utils import *
 from darknet import Darknet
 import cv2
+import time
+import mipso_custom
 
-def demo(cfgfile, weightfile):
+def demo(cfgfile, weightfile, in_file, batch=1, out_path=None, loop=False):
     m = Darknet(cfgfile)
     m.print_network()
     m.load_weights(weightfile)
@@ -16,37 +18,97 @@ def demo(cfgfile, weightfile):
         namesfile = 'data/names'
     class_names = load_class_names(namesfile)
  
-    use_cuda = 1
+    use_cuda = 0
     if use_cuda:
         m.cuda()
 
-    cap = cv2.VideoCapture(0)
-    if not cap.isOpened():
-        print("Unable to open camera")
-        exit(-1)
-
-    while True:
-        res, img = cap.read()
-        if res:
-            sized = cv2.resize(img, (m.width, m.height))
-            bboxes = do_detect(m, sized, 0.5, 0.4, use_cuda)
-            print('------')
+    capture = mipso_custom.mipso_opencv_capture(in_file)
+    pauseMode = None
+    image_data_batch = []
+    capture_image = []
+    first_capture = 1
+    firstShow = True
+
+    while len(capture) > 0:
+        for cap in capture:
+            res, img = cap.read()
+            if loop and img is None:
+                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
+                _, img = cap.read()
+            if first_capture and out_path is not None:
+                assert (len(capture) == 1), 'saving video works only with single input'
+                out_image = []
+                height, width, _ = img.shape
+                fourcc = cv2.VideoWriter_fourcc(*'XVID')
+                fps = max(1, round(capture[0].get(cv2.CAP_PROP_FPS)))
+                videoWriter = cv2.VideoWriter(out_path, fourcc, fps, (width, height))
+                first_capture = 0
+            if img is None:
+                capture = []
+                if len(image_data_batch) > 0:
+                    batch = 1
+            else:
+                capture_image.append(img)
+                sized = cv2.resize(img, (m.width, m.height))
+                image_data_batch.append(sized)
+
+        if len(image_data_batch) < batch:
+            continue
+
+        start_inf = time.time()
+        boxes_l = do_detect(m, image_data_batch, 0.5, 0.4, use_cuda)
+        took_inf = time.time() - start_inf
+        print('------')
+        for idx, (img, bboxes) in enumerate(zip(capture_image, boxes_l)):
             draw_img = plot_boxes_cv2(img, bboxes, None, class_names)
-            cv2.imshow(cfgfile, draw_img)
-            cv2.waitKey(1)
+            mipso_custom.mipso_window_info(draw_img, idx, "PYT YOLOV2", took_inf)
+            if out_path is None:
+                if firstShow:
+                    height, width, _ = draw_img.shape
+                    mipso_custom.mipso_window_init(idx, width, height)
+                cv2.imshow(str(idx), draw_img)
+            else:
+                out_image.append(draw_img)
+        firstShow = False
+        image_data_batch = []
+        capture_image = []
+        if out_path is None:
+            choice = cv2.waitKey(1)
         else:
-             print("Unable to read image")
-             exit(-1) 
+            choice = 0
+        choice = mipso_custom.mipso_process_key(choice)
+        if choice == 27: break
+
+    if out_path is not None:
+        print("No more input, encoding capture...")
+        for v in out_image:
+            videoWriter.write(v)
+        videoWriter.release()
 
 ############################################
 if __name__ == '__main__':
-    if len(sys.argv) == 3:
+    if len(sys.argv) >= 4:
         cfgfile = sys.argv[1]
         weightfile = sys.argv[2]
-        demo(cfgfile, weightfile)
+        in_file = sys.argv[3]
+        if len(sys.argv) >= 5:
+            batch = int(sys.argv[4])
+        else:
+            batch = 1
+        if len(sys.argv) >= 6:
+            loop = sys.argv[5] == '1'
+        else:
+            loop = False
+        if len(sys.argv) >= 7:
+            out_path = sys.argv[6]
+        else:
+            out_path = None
+        demo(cfgfile, weightfile, in_file, batch, out_path, loop)
         #demo('cfg/tiny-yolo-voc.cfg', 'tiny-yolo-voc.weights')
     else:
         print('Usage:')
-        print('    python demo.py cfgfile weightfile')
+        print('    python demo.py cfgfile weightfile input_list [batchsize] [loop] [output_video]')
         print('')
-        print('    perform detection on camera')
+        print('    perform detection on multiple camera or multiple video')
+        print('    batch size will be the number of inputs')
+        print('    for single input, batch size can be forced and output saved on a video')
diff --git a/detect.py b/detect.py
index c20b009..0a47539 100644
--- a/detect.py
+++ b/detect.py
@@ -19,7 +19,7 @@ def detect(cfgfile, weightfile, imgfile):
     else:
         namesfile = 'data/names'
     
-    use_cuda = 1
+    use_cuda = 0
     if use_cuda:
         m.cuda()
 
@@ -51,7 +51,7 @@ def detect_cv2(cfgfile, weightfile, imgfile):
     else:
         namesfile = 'data/names'
     
-    use_cuda = 1
+    use_cuda = 0
     if use_cuda:
         m.cuda()
 
@@ -85,7 +85,7 @@ def detect_skimage(cfgfile, weightfile, imgfile):
     else:
         namesfile = 'data/names'
     
-    use_cuda = 1
+    use_cuda = 0
     if use_cuda:
         m.cuda()
 
diff --git a/mipso_custom.py b/mipso_custom.py
new file mode 100644
index 0000000..69a97ef
--- /dev/null
+++ b/mipso_custom.py
@@ -0,0 +1,232 @@
+
+import cv2
+import os
+
+############################################################
+############################################################
+
+# the application will open several window and place them on the desktop at
+# specific location. The below variables allow to configure the location
+
+# this is the X/Y offset for window position
+WINDOW_X_OFFSET         = int(os.environ.get("WINDOW_X_OFFSET")         or 0)
+WINDOW_Y_OFFSET         = int(os.environ.get("WINDOW_Y_OFFSET")         or 0)
+
+# this is the number of window per row
+WINDOW_PER_ROW          = int(os.environ.get("WINDOW_PER_ROW")          or 3)
+
+# this is a scaling factor to reduce the window dimension (2: window will be 2x smaller)
+WINDOW_RATIO            = float(os.environ.get("WINDOW_RATIO")          or 1)
+
+# this force the width and height of the video
+WINDOW_WIDTH            = int(os.environ.get("WINDOW_WIDTH")            or 0)
+WINDOW_HEIGHT           = int(os.environ.get("WINDOW_HEIGHT")           or 0)
+
+# this is the index position to place a window so a slide can be displayed. For instance, in a 3x3 configuration use 4 to let the center empty
+WINDOW_SLIDE_IDX        = int(os.environ.get("WINDOW_SLIDE_IDX")        or 4)
+
+# this is the number of batch (usually the number of cores) per system
+BATCH_PER_SYSTEM        = int(os.environ.get("BATCH_PER_SYSTEM")        or 1)
+
+# this is the number of system per board
+SYSTEM_PER_BOARD        = int(os.environ.get("SYSTEM_PER_BOARD")        or 2)
+
+# show info/FPS by default
+SHOW_INFO               = int(os.environ.get("SHOW_INFO")               or 0)
+SHOW_FPS                = int(os.environ.get("SHOW_FPS")                or 0)
+
+
+# enable the MJPG mode
+OPENCV_CAMERA_MJPG      = int(os.environ.get("OPENCV_CAMERA_MJPG")      or 1)
+# force camera width/height
+OPENCV_CAMERA_WIDTH     = int(os.environ.get("OPENCV_CAMERA_WIDTH")     or 0)
+OPENCV_CAMERA_HEIGHT    = int(os.environ.get("OPENCV_CAMERA_HEIGHT")    or 0)
+
+
+############################################################
+############################################################
+
+# only for YoloV2:
+def mipso_get_interpolation():
+    return cv2.INTER_NEAREST
+    return cv2.INTER_LINEAR
+    #return cv2.INTER_LANCZOS4
+
+# only for YoloV2
+def mipso_camera_reduce_delay():
+    return False # normal execution: capture / inference / display from capture
+    return True # reduce delay execution: inference on prev capture / capture / display from capture using previous inference
+
+# only for YoloV2
+def mipso_box_rounding(i):
+    return i
+    round = 20
+    return ((i + round) // (2*round) ) * 2*round
+
+def mipso_camera_init(c):
+    print ("Initial camera resolution {}x{} @ {}".format(
+        c.get(cv2.CAP_PROP_FRAME_WIDTH),
+        c.get(cv2.CAP_PROP_FRAME_HEIGHT),
+        c.get(cv2.CAP_PROP_FPS)))
+    if OPENCV_CAMERA_WIDTH != 0:
+        c.set(cv2.CAP_PROP_FRAME_WIDTH , OPENCV_CAMERA_WIDTH)
+    if OPENCV_CAMERA_HEIGHT != 0:
+        c.set(cv2.CAP_PROP_FRAME_HEIGHT , OPENCV_CAMERA_HEIGHT)
+    if OPENCV_CAMERA_MJPG != 0:
+        c.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M', 'J', 'P', 'G'))
+    print ("Using camera resolution {}x{} @ {}".format(
+        c.get(cv2.CAP_PROP_FRAME_WIDTH),
+        c.get(cv2.CAP_PROP_FRAME_HEIGHT),
+        c.get(cv2.CAP_PROP_FPS)))
+
+def mipso_window_init(idx, width, height):
+    cv2.namedWindow(str(idx), cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_FREERATIO)
+    #cv2.namedWindow(str(idx), cv2.WINDOW_NORMAL)
+
+    if WINDOW_WIDTH != 0 and WINDOW_HEIGHT != 0:
+        cur_width = WINDOW_WIDTH
+        cur_height = WINDOW_HEIGHT
+    else:
+        cur_width = int(width // WINDOW_RATIO)
+        cur_height = int(height // WINDOW_RATIO)
+
+    cv2.resizeWindow(str(idx), (cur_width, cur_height))
+    board=0
+    system=0
+    core=0
+    if 'ZEBRA_RUNSESSION_ENABLECORES' in os.environ:
+        c = os.environ['ZEBRA_RUNSESSION_ENABLECORES'].split(':')[0].split('_')
+        if len(c[0]) > 0:
+            board = int(c[0][1:])
+        if len(c) > 1:
+            system = int(c[1][1:])
+        if len(c) > 2:
+            core = int(c[2][1:])
+
+    cur_idx = idx + core + BATCH_PER_SYSTEM * ( system + SYSTEM_PER_BOARD * board)
+    # in case we want to put slide somewhere
+    if cur_idx >= WINDOW_SLIDE_IDX:
+        cur_idx = cur_idx + 1
+    pos_x = WINDOW_X_OFFSET + int( (cur_idx % WINDOW_PER_ROW)*cur_width)
+    pos_y = WINDOW_Y_OFFSET + int( 0 + (cur_idx // WINDOW_PER_ROW)*cur_height)
+
+    print("Move window {} {}x{} for system {} @ ({} , {})".format(idx, cur_width, cur_height, system, pos_x, pos_y))
+    cv2.moveWindow(str(idx), pos_x, pos_y)
+
+def mipso_parse_input(file):
+    mode='file'
+    for part in file.split(':'):
+        if 'camera' == part:
+            mode='camera'
+            continue
+        if part.isdigit():
+            yield mode, int(part)
+        else:
+            mode='file'
+            yield mode, part
+
+
+
+def mipso_opencv_capture(file):
+    """
+    return a list of opencv capture device
+    """
+    camera = []
+    # TODO: use a better handling of the different mode (class)
+    if 'camera' == file:
+        # open all camera
+        camera_idx=0
+        while True:
+            c = cv2.VideoCapture(camera_idx)
+            if c.isOpened() == False:
+                break
+            camera.append(c)
+            mipso_camera_init(c)
+            camera_idx = camera_idx + 1
+    else:
+        for mode, url in mipso_parse_input(file):
+            c = cv2.VideoCapture(url)
+            if c.isOpened() == False:
+                print("Error opening source {} {}".format(mode, url))
+                break
+            if mode == 'camera':
+                mipso_camera_init(c)
+            camera.append(c)
+
+    assert len(camera) > 0, 'Cannot capture source'
+    return camera
+
+# TODO: create a Mipso class
+showHelp = False
+pauseMode = False
+showWindowInfo = SHOW_INFO != 0
+showFps = SHOW_FPS != 0
+
+def mipso_process_key(key):
+    global showHelp
+    global pauseMode
+    global showWindowInfo
+    global showFps
+
+    if pauseMode:
+        print ("pause mode, press 'p' to play or any other key to move to the next frame")
+    while True:
+        if key == 27:
+            return key
+        if key > 0 and chr(key) in 'h':
+            showHelp = not showHelp
+        if key > 0 and chr(key) in 'p':
+            pauseMode = not pauseMode
+        if key > 0 and chr(key) in 'i':
+            showWindowInfo = not showWindowInfo
+        if key > 0 and chr(key) in 'f':
+            showFps = not showFps
+        if not pauseMode or key != -1:
+            break
+        key = cv2.waitKey(1)
+    return key
+
+def print_help(frame):
+
+    HELP="\
+Key usage:\n\
+    ESC : exit the demo\n\
+    h   : toggle displaying this help\n\
+    p   : pause the video (any other key will do frame by frame)\n\
+    i   : toggle showing network information\n\
+    f   : toggle showing FPS\n\
+"
+    scale = 1
+    if frame.shape[1] < 1240:
+        scale = scale * frame.shape[1] / 1240
+    for y, line in enumerate(HELP.split('\n')):
+        cv2.putText(frame, line, (20, int(scale*(200 + y * 40))), cv2.FONT_HERSHEY_SIMPLEX,
+                    scale, (255,255,0), 1)
+
+
+def print_text(frame, x, y, text, scale=1.2):
+    font                   = cv2.FONT_HERSHEY_SIMPLEX
+    bottomLeftCornerOfText = (10,500)
+    fontScale              = scale
+    fontColor              = (0,0,255)
+    lineType               = 2 if scale >= 1 else 1
+
+    cv2.putText(frame, text, (x, y),
+        font,
+        fontScale,
+        fontColor,
+        lineType)
+
+def mipso_window_info(f, idx, name, latency):
+    if showHelp:
+        print_help(f)
+    if showWindowInfo:
+        scale = 1
+        if f.shape[1] < 500:
+            scale = scale * f.shape[1] / 500
+        if showFps:
+            print_text( f, 10, int(scale*30), "Zebra: {} FPS={}".format(name, str(int(1/latency))), scale )
+        else:
+            print_text( f, 10, int(scale*30), "Zebra: {}".format(name), scale)
+        #print_text(f, 10, 50, "Latency is: " + str(int(1000*latency)) + " ms. FPS is: " + str(int(1/latency)) + " img/s")
+
diff --git a/run b/run
new file mode 100755
index 0000000..0b6266f
--- /dev/null
+++ b/run
@@ -0,0 +1,19 @@
+#! /bin/bash
+
+batch=1
+loop=0
+out_file=
+
+in="$1"
+shift
+
+while [ $# -gt 0 ]
+do
+  [ "$1" = "--batch" ] && batch=$2 && shift 2 && continue
+  [ "$1" = "--loop" ] && loop=1 && shift && continue
+  [ "$1" = "--out_file" ] && out_file=$2 && shift 2 && continue
+  shift
+done
+
+python3 demo.py cfg/yolo.cfg yolo.weights "$in" $batch $loop $out_file
+
diff --git a/utils.py b/utils.py
index abecff3..8b14172 100644
--- a/utils.py
+++ b/utils.py
@@ -9,6 +9,8 @@ from torch.autograd import Variable
 
 import struct # get_image_size
 import imghdr # get_image_size
+if (os.getenv('ZEBRA_INSTALL_DIR')):
+    from zebraRunner import zebra
 
 def sigmoid(x):
     return 1.0/(math.exp(-x)+1.)
@@ -111,7 +113,7 @@ def convert2cpu_long(gpu_matrix):
     return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)
 
 def get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors, only_objectness=1, validation=False):
-    anchor_step = len(anchors)/num_anchors
+    anchor_step = len(anchors)//num_anchors
     if output.dim() == 3:
         output = output.unsqueeze(0)
     batch = output.size(0)
@@ -123,15 +125,15 @@ def get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors, onl
     all_boxes = []
     output = output.view(batch*num_anchors, 5+num_classes, h*w).transpose(0,1).contiguous().view(5+num_classes, batch*num_anchors*h*w)
 
-    grid_x = torch.linspace(0, w-1, w).repeat(h,1).repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w).cuda()
-    grid_y = torch.linspace(0, h-1, h).repeat(w,1).t().repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w).cuda()
+    grid_x = torch.linspace(0, w-1, w).repeat(h,1).repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w)
+    grid_y = torch.linspace(0, h-1, h).repeat(w,1).t().repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w)
     xs = torch.sigmoid(output[0]) + grid_x
     ys = torch.sigmoid(output[1]) + grid_y
 
     anchor_w = torch.Tensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([0]))
     anchor_h = torch.Tensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([1]))
-    anchor_w = anchor_w.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w).cuda()
-    anchor_h = anchor_h.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w).cuda()
+    anchor_w = anchor_w.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w)
+    anchor_h = anchor_h.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w)
     ws = torch.exp(output[2]) * anchor_w
     hs = torch.exp(output[3]) * anchor_h
 
@@ -207,10 +209,10 @@ def plot_boxes_cv2(img, boxes, savename=None, class_names=None, color=None):
     height = img.shape[0]
     for i in range(len(boxes)):
         box = boxes[i]
-        x1 = int(round((box[0] - box[2]/2.0) * width))
-        y1 = int(round((box[1] - box[3]/2.0) * height))
-        x2 = int(round((box[0] + box[2]/2.0) * width))
-        y2 = int(round((box[1] + box[3]/2.0) * height))
+        x1 = int(((box[0] - box[2]/2.0) * width))
+        y1 = int(((box[1] - box[3]/2.0) * height))
+        x2 = int(((box[0] + box[2]/2.0) * width))
+        y2 = int(((box[1] + box[3]/2.0) * height))
 
         if color:
             rgb = color
@@ -313,6 +315,7 @@ def do_detect(model, img, conf_thresh, nms_thresh, use_cuda=1):
     model.eval()
     t0 = time.time()
 
+    listInput = False
     if isinstance(img, Image.Image):
         width = img.width
         height = img.height
@@ -322,6 +325,9 @@ def do_detect(model, img, conf_thresh, nms_thresh, use_cuda=1):
         img = img.float().div(255.0)
     elif type(img) == np.ndarray: # cv2 image
         img = torch.from_numpy(img.transpose(2,0,1)).float().div(255.0).unsqueeze(0)
+    elif type(img) == list:
+        listInput = True
+        img = torch.from_numpy(np.asarray(img).transpose(0,3,1,2)).float().div(255.0)
     else:
         print("unknow image type")
         exit(-1)
@@ -333,19 +339,22 @@ def do_detect(model, img, conf_thresh, nms_thresh, use_cuda=1):
     img = torch.autograd.Variable(img)
     t2 = time.time()
 
-    output = model(img)
+    if (os.getenv('ZEBRA_INSTALL_DIR')):
+        output = zebra.run(model, img)
+    else:
+        output = model(img)
     output = output.data
     #for j in range(100):
     #    sys.stdout.write('%f ' % (output.storage()[j]))
     #print('')
     t3 = time.time()
 
-    boxes = get_region_boxes(output, conf_thresh, model.num_classes, model.anchors, model.num_anchors)[0]
+    boxes = get_region_boxes(output, conf_thresh, model.num_classes, model.anchors, model.num_anchors)
     #for j in range(len(boxes)):
     #    print(boxes[j])
     t4 = time.time()
 
-    boxes = nms(boxes, nms_thresh)
+    boxes = [nms(b, nms_thresh) for b in boxes]
     t5 = time.time()
 
     if False:
@@ -357,7 +366,10 @@ def do_detect(model, img, conf_thresh, nms_thresh, use_cuda=1):
         print('             nms : %f' % (t5 - t4))
         print('           total : %f' % (t5 - t0))
         print('-----------------------------------')
-    return boxes
+    if listInput:
+        return boxes
+    else:
+        return boxes[0]
 
 def read_data_cfg(datacfg):
     options = dict()
